# Lessons Learned

Curated learnings from generating and sharing diagnostics. This document is referenced by Cursor during generation via `.cursor/rules/lessons-learned.mdc`.

Last reviewed: 2026-02-19

---

## NRR + CX Diagnostic (Basic)

### What works well
- Peer NRR benchmarks are the first thing clients notice — make sure the peer set is tight and defensible
- Journey map pain points that reference specific product names or features land much harder than generic ones

### Common pitfalls
- (No entries yet — add from feedback/)

### Client reactions
- (No entries yet — add from feedback/)

---

## NRR Growth Diagnostic (Enhanced)

### What works well
- The NRR waterfall is consistently cited as the most compelling visual — lead with it
- Value-at-stake scenarios that quantify dollar impact get executive attention

### Common pitfalls
- (No entries yet — add from feedback/)

### Client reactions
- (No entries yet — add from feedback/)

---

## Customer Operations Assessment

### What works well
- Role-by-role impact analysis gives CS/PS/Support leaders something immediately actionable
- Agentic readiness framing positions McKinsey distinctively vs. generic consulting teardowns

### Common pitfalls
- (No entries yet — add from feedback/)

### Client reactions
- (No entries yet — add from feedback/)

---

## Cross-cutting Learnings

### Prompting tips
- The more specific context you give Cursor (hypotheses, sector nuance, competitive dynamics), the higher quality the output
- Always review the peer set before publishing — incorrect peers undermine credibility instantly

### Data gathering
- (No entries yet — add from feedback/)

### Deployment
- (No entries yet — add from feedback/)
