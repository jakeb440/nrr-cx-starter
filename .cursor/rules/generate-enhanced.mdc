# Generate NRR CX Enhanced Diagnostic

When the user asks to generate an NRR CX Enhanced diagnostic, follow this process.

## What Enhanced Includes

Everything in Basic PLUS (reference: https://nrr-cx-databricks-enhanced.vercel.app/):

1. **NRR Benchmark** — Vertical bar chart with quartile coloring (Q1=green, Q2=blue, Q3=amber, Q4=red), estimated/implied opacity (60% for estimated), per-peer methodology notes (always visible, not collapsible), multi-period trend
2. **Management Commentary** — Key quotes in responsive grid, **collapsible and defaulting to closed**
3. **Current-State Customer Journey** — 6-8 stages with color-coded stage bubbles (green=net positive, amber=mixed, red=net negative), each finding attributed with source and date
4. **NRR Maturity Assessment** — 6 dimensions rated on a Basic → Advanced → Next-gen scale
5. **NRR Waterfall** — Decomposition of current NRR into retention/expansion components with 2-3yr improvement targets
6. **Value at Stake** — Current, Realistic, and Aspirational NRR improvement scenarios with incremental ARR and EV gain
7. **Highest-Impact Actions** — 5 actions with pp NRR impact, timeline, dimension mapping, and detailed rationale
8. **Diagnostic Synthesis** — Narrative + side-by-side Top Strengths and Top Risks with collapsible detail cards. **Concise** — 1 focused paragraph per strength/risk.

## The 6 NRR Maturity Dimensions

These come from the McKinsey NRR Maturity Benchmark (N=101 B2B SaaS companies):

1. **Segment & Cover** — Account segmentation, coverage model, resource allocation
2. **Design Journeys Customer-Back** — Customer journey design, onboarding, time-to-value
3. **Predict & Preempt Health** — Health scoring, churn prediction, proactive intervention
4. **Pricing, Packaging & Policies** — Pricing model clarity, packaging, renewal policies
5. **Org & Talent Engine** — CS org structure, talent, roles, AI readiness
6. **Equip the Frontline** — Tools, playbooks, data access for frontline teams

Rate each as transitions: "Basic→Advanced" or "Advanced→Next-gen" based on evidence.

## NRR Waterfall Construction

Decompose NRR into:
- **Gross retention** (logos retained × ARR retained)
- **Expansion** (upsell + cross-sell + price increases)
- **Contraction** (downgrades + partial churn)

Show current NRR and target NRR (2-3 year) with the improvement delta.

## Value at Stake Scenarios

| Scenario | NRR Target | How to derive |
|---|---|---|
| Current | Actual NRR | Reported figure |
| Realistic | +3-5pp | Achievable with focused CX improvements, 12-18 months |
| Aspirational | +8-12pp | Requires structural changes, 2-3 years |

For each scenario, calculate:
- **Incremental ARR** = Base ARR × (Target NRR - Current NRR) / 100
- **EV Multiple** = Current or re-rated based on NRR improvement
- **Implied EV Gain** = Incremental ARR × EV Multiple + (New EV Multiple - Current Multiple) × Current Revenue

Explain the EV multiple rationale for each scenario.

## Highest-Impact Actions

Generate exactly 5 actions. Each must have:
- **Title** — Actionable, specific (not generic "improve customer experience")
- **NRR impact** — Stated in pp (e.g., "+2pp NRR")
- **Timeline** — e.g., "6-12 months"
- **Dimension** — Which maturity dimension it maps to
- **Rationale** — 1-2 paragraphs explaining the mechanism, with supporting evidence/quotes

The 5 actions should sum to the realistic scenario improvement target.

Include a note explaining that actions are uniquely weighted and how they sum.

## Data Gathering

### Required — NRR Benchmarking
- **Client name** and sector/sub-sector
- **Client NRR** (latest reported or implied from revenue growth)
- **Peer set** (5-7 companies in the same sub-sector)
- **Peer NRR figures** (from SEC filings, earnings calls, or NRR Tracker at `nrr-tracker/data/`)
- **Multiple periods** of NRR data if available (4 quarters/years for trend)
- **ALL NRR FIGURES MUST BE THE MOST RECENT AVAILABLE.** Check the latest quarterly earnings for every peer. If a company reported results in the last 3 months, use those numbers. Cite the specific reporting date in the methodology note. Stale data undermines credibility — a client will notice if you're using a figure from 2024 when a 2025 figure exists.
- **Per-peer methodology**: For EVERY peer (including the target), document how NRR was sourced:
  - `"CompanyName: Direct NRR disclosure from Q3 FY2026 earnings call (December 2, 2025)."` (if reported)
  - `"CompanyName: Implied from X% revenue growth and Y% ARR growth. Company does not disclose NRR directly."` (if estimated)
  - Include the calculation logic, data source, specific reporting date, and any caveats
  - If a company has stopped reporting NRR as a metric, note this explicitly and explain how you derived the estimate

### For Journey Map — Voice of Customer ONLY

- **Source exclusively from customer-experience data**: G2 reviews, Gartner Peer Insights ratings, TrustRadius reviews, Reddit threads (r/sysadmin, r/devops, product-specific subs), Capterra, App Store reviews, Trustpilot
- **Every strength and pain point must be grounded in what customers actually say** — real user sentiment, verbatim language patterns, and review themes
- **DO NOT include financial results, revenue metrics, or investor-facing data in journey stages.** The journey map is about the customer's lived experience, not the company's financial performance. Financial data belongs in the NRR Benchmark, Waterfall, Value at Stake, and Synthesis sections only.
- For each finding, note whether it's unique to the client, competitors do better, or industry-wide

### Journey Finding Attribution (REQUIRED)

**Every strength and pain point MUST have a `source` field** attributed in the format:

```
"G2 review, February 2026"
"Gartner Peer Insights review, November 2025"
"Reddit r/sysadmin, January 2026"
"Trustpilot review, December 2025"
"TrustRadius review, October 2025"
```

Format: `"{Platform} review, {Month} {Year}"`. Use the most specific, most recent date available. Do not use ranges like "2024–2025" or vague labels like "Multiple Reviewers." Each attribution should look like a single review citation.

### For Synthesis
- Identify the 3 most important strengths driving NRR
- Identify the 3 biggest risks to NRR
- **Keep synthesis concise**: each strength/risk gets a short punchy title and **1 focused paragraph** of evidence (not 2-3). Pack the key insight, evidence, and implication into one tight paragraph.
- The synthesis CAN and SHOULD reference financial data alongside CX findings — this is where the two worlds connect

## Data Schema

The Enhanced `client-data.json` extends Basic with additional fields:

```json
{
  "company": { "name": "...", "sector": "...", "financials": "..." },
  "nrr": {
    "current": 140,
    "currentPeriod": "Q4 FY2026",
    "quartile": "Q1",
    "history": [{ "period": "FY2025", "nrr": 135 }],
    "peers": { "count": 6, "median": 122.5, "topQuartile": 128.8, "bottomQuartile": 118.5, "range": "115–140%" },
    "peerData": [
      { "company": "PeerName", "nrr": 125, "period": "Q3 FY2026", "isTarget": false, "isEstimated": true }
    ],
    "methodologyNotes": [
      "PeerName: Implied from 25% revenue growth (Q3 FY2026 earnings, November 2025). Does not formally disclose NRR."
    ],
    "managementCommentary": [
      { "quote": "Quote text...", "source": "CEO, Q4 FY2026 Earnings Call" }
    ],
    "maturity": [
      { "dimension": "Segment & Cover", "current": "Advanced", "target": "Next-gen", "label": "Advanced→Next-gen" }
    ],
    "waterfall": {
      "currentNRR": 140,
      "targetNRR": 145.5,
      "improvement": 5.5,
      "components": {
        "grossRetention": { "current": 95, "target": 96 },
        "expansion": { "current": 50, "target": 53 },
        "contraction": { "current": -5, "target": -3.5 }
      }
    },
    "valueAtStake": {
      "baseARR": "$5.4B",
      "currentMultiple": 25,
      "methodology": "Explanation of current multiple derivation...",
      "scenarios": [
        {
          "name": "Realistic",
          "targetNRR": 145,
          "incrementalARR": "$270M/yr",
          "evMultiple": 27,
          "evGain": "$18.1B",
          "rationale": "Explanation..."
        }
      ]
    },
    "actions": [
      {
        "rank": 1,
        "title": "Specific actionable title",
        "impact": "+2pp NRR",
        "timeline": "6-12 months",
        "dimension": "Predict & Preempt",
        "rationale": "Detailed explanation with evidence..."
      }
    ]
  },
  "journey": {
    "description": "ClientName — strengths and pain points across the B2B CX journey",
    "stages": [
      {
        "number": 1,
        "name": "Stage Name",
        "description": "What happens in this stage",
        "strengths": [
          {
            "text": "Voice-of-customer finding with verbatim quote or data point",
            "severity": "high",
            "source": "G2 review, January 2026"
          }
        ],
        "painPoints": [
          {
            "text": "'Verbatim quote' — context and implication",
            "severity": "high",
            "source": "Gartner Peer Insights review, November 2025"
          }
        ],
        "competitiveContext": [{ "label": "Context note", "type": "unique" }]
      }
    ]
  },
  "synthesis": {
    "narrative": "2-3 paragraph synthesis...",
    "topStrengths": [{ "title": "Short punchy title", "detail": "One focused paragraph with key evidence." }],
    "topRisks": [{ "title": "Short punchy title", "detail": "One focused paragraph with key evidence." }]
  }
}
```

## Scraping Pipeline (Optional)

For Enhanced diagnostics that use the CX Teardown pipeline:

```bash
cd templates/enhanced

# Set up Python environment
python3 -m venv venv && source venv/bin/activate
pip install -r requirements.txt

# Configure API keys
cp .env.example .env  # Edit with your keys

# Run scraping + analysis
python cx_teardown.py \
  --company "Company Name" \
  --competitors "Comp1,Comp2,Comp3" \
  --industry b2b-enterprise-software
```

### Scraping Best Practices

**B2B Enterprise Software:** G2, Gartner Peer Insights, Reddit (r/sysadmin, r/devops, product-specific subs)
**B2C:** App Store, Trustpilot, Reddit consumer subs

- Target 50-200 reviews per source, prioritize last 12-18 months
- Skip reviews shorter than 50 words
- After automated scraping, manually add 5-10 quotes from earnings calls and analyst reports
- Use the industry taxonomy YAML to classify reviews

## Quality Checklist (in addition to Basic checklist)

- [ ] **All NRR figures use the most recent available data** — check latest quarterly earnings for every peer
- [ ] NRR figures verified against SEC filings or NRR Tracker
- [ ] Peer set is appropriate (same sub-sector, comparable scale)
- [ ] **Every peer has a named methodology note** with specific reporting date and calculation logic
- [ ] Every journey stage has at least 1 strength or pain point
- [ ] **Journey content is 100% voice-of-customer** — no financial results in journey stages
- [ ] **Every journey finding has a `source` field** in the format "Platform review, Month Year"
- [ ] All quotes are real and sourced (never fabricated)
- [ ] Management commentary quotes are from actual earnings calls
- [ ] **Synthesis is concise** — 1 paragraph per strength/risk, not 2-3
- [ ] NRR maturity ratings are supported by evidence
- [ ] Waterfall components sum correctly to total NRR
- [ ] Value-at-stake math is internally consistent
- [ ] EV multiples are reasonable (cite comp company multiples)
- [ ] 5 actions sum to the realistic improvement target
- [ ] Each action has specific, non-generic language
- [ ] Actions cite real customer/market evidence

## Skeptical Client Review (REQUIRED before publishing)

After generating the full diagnostic, perform a **skeptical client review pass**. Read through the entire output as if you are the client's VP of Customer Success — someone who knows this business intimately and will push back on anything that feels wrong, generic, or unsupported.

### What to check:
1. **Peer set credibility** — Would the client agree these are the right peers? Are any peers from a different sub-sector, different scale, or different business model that would undermine the comparison?
2. **NRR accuracy and recency** — Are any NRR figures stale, wrong, or based on flawed logic? Would a client who tracks their own NRR daily spot an error? Is every figure from the most recent available reporting period?
3. **Journey specificity** — Do the journey findings feel specific to THIS company, or could they apply to any B2B SaaS company? Replace generic findings with specific ones. Every finding should reference a real platform, reviewer, or review theme.
4. **Journey attributions** — Does every finding have a source in "Platform review, Month Year" format? Would a reader trust these as real citations?
5. **Strength/risk balance** — Are the strengths genuinely strong (not just "they have a product")? Are the risks genuinely risky (not just "competition exists")?
6. **Quote quality** — Would a client read a management quote and say "that's taken out of context" or "that's from 2 years ago"? Use the most recent, most relevant quotes.
7. **Maturity ratings** — Would the client's CS leader agree with each dimension rating, or would they push back? If a rating seems generous or harsh without clear evidence, adjust it.
8. **Action specificity** — Are the highest-impact actions things the client could actually DO, or are they generic consulting advice? "Build a FinOps program" is better than "improve customer experience."
9. **Value-at-stake credibility** — Are the EV multiples defensible? Would the client's CFO challenge the math? Cite comparable company multiples explicitly.
10. **Tone** — Is the language appropriately analytical and respectful? Nothing condescending, nothing that reads as obvious padding.

### What to fix:
- Replace any finding a skeptical client would challenge
- Tighten language that feels vague or hedged
- Add specificity where the content feels generic
- Remove or rewrite any section where confidence is low
- If an action or maturity rating cannot be substantiated, revise it rather than leaving it unsupported

Document any changes made during this review pass.

## Deployment

```bash
cd output/nrr-cx-{client}-enhanced
npm install
npm run build
npx vercel --prod
```

Then update `diagnostics.json` with the new entry.
